\documentclass[aspectratio=169,10pt,t]{beamer}
% \usetheme[
% %%% options passed to the outer theme
% %    progressstyle=fixedCircCnt,   %either fixedCircCnt, movCircCnt, or corner
% %    rotationcw,          % change the rotation direction from counter-clockwise to clockwise
% %    shownavsym          % show the navigation symbols
%   ]{SDUsimple}
\usepackage{SDUtheme/beamerthemeSDUsimple}
% If you want to change the colors of the various elements in the theme, edit and uncomment the following lines
% Change the bar and sidebar colors:
%\setbeamercolor{SDUsimple}{fg=red!20,bg=red}
%\setbeamercolor{sidebar}{bg=red!20}
% Change the color of the structural elements:
%\setbeamercolor{structure}{fg=red}
% Change the frame title text color:
%\setbeamercolor{frametitle}{fg=blue}
% Change the normal text color background:
%\setbeamercolor{normal text}{fg=black,bg=gray!10}
% ... and you can of course change a lot more - see the beamer user manual.
\usepackage{color}
\usepackage{float}
\usepackage{dsfont}                         % Enables double stroke fonts
\usepackage{bm}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{todonotes}
\usepackage{color}
% \usepackage{listings}
% \usepackage[ruled,vlined]{algorithm2e}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.
\usepackage{helvet}
\usefonttheme{professionalfonts}

\newtheorem{algorithm}{Algorithm}
%\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
% colored hyperlinks
\newcommand{\chref}[2]{%
  \href{#1}{{\usebeamercolor[bg]{SDUsimple}#2}}%
}

\title{Machine (or Reinforcement): Learning to assist vessel docking in extreme environments}
\subtitle{Bachelor defense}
%\date{\today}
\date{\today }

\author{
  \textbf{Anne-Charlotte Poulsen}
}

% - Give the names in the same order as they appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation. See the beamer manual for an example

\institute[
%  {\includegraphics[scale=0.2]{SDU_segl}}\\ %insert a company, department or university logo
  SDU Robotics\\
  The Maersk Mc-Kinney Moller Institute\\
  University of Southern Denmark
] % optional - is placed in the bottom of the sidebar on every slide
{% is placed on the bottom of the title page
  SDU Robotics\\
  The Maersk Mc-Kinney Moller Institute\\
  University of Southern Denmark

  %there must be an empty line above this line - otherwise some unwanted space is added between the university and the country (I do not know why;( )
}

% specify a logo on the titlepage (you can specify additional logos an include them in
% institute command below
\pgfdeclareimage[height=0.5cm]{titlepagelogo}{SDUgraphics/SDU_logo_new} % placed on the title page
%\pgfdeclareimage[height=1.5cm]{titlepagelogo2}{SDUgraphics/SDU_logo_new} % placed on the title page
\titlegraphic{% is placed on the bottom of the title page
  \pgfuseimage{titlepagelogo}
%  \hspace{1cm}\pgfuseimage{titlepagelogo2}
}

\begin{document}
% the titlepage
{\SDUwavesbg%
\begin{frame}[plain,noframenumbering] % the plain option removes the header from the title page
  \titlepage
\end{frame}}
%%%%%%%%%%%%%%%%

% TOC
% \begin{frame}{Agenda}{\vphantom{(y}}
% \tableofcontents
% \end{frame}
%%%%%%%%%%%%%%%
\begin{frame}{Agenda}{\vphantom{(y}}
  %% Needed if you want to use table of content instead of itemize.
  % \section{Introduction}
  % \section{Reinforcement Learning}
  % \section{Simplified model}
  % \section{Results \& Discussion}
  % \section{Conclusion \& Future work}
  \begin{itemize}
    \item Quick introduction to the problem
    \item Reinforcement Learning
    \item Simplified model
    \item Results \& Discussion
    \item Conclusion \& Future work
  \end{itemize}
\end{frame}

\begin{frame}{Introductory}{Overview\vphantom{(y}}
\vspace{-0.7em}
Overview of the problem
\begin{itemize}
\item Vessel stabilizer
\item Why is a vessel stabilizer needed?
\item What is Dacoma's current solution?
\item The objective
\item The approach
\end{itemize}
\end{frame}

\begin{frame}{Introductory}{Vessel stabilizer\vphantom{(y}}
\vspace{-0.7em}
Overview of the problem
\begin{itemize}
\item {\color{blue}Vessel stabilizer}
\item Why is a vessel stabilizer needed?
\item What is Dacoma's current solution?
\item The objective
\item The approach
\end{itemize}
\end{frame}

\begin{frame}{Introductory}{Vessel stabilizer\vphantom{(y}}
\vspace{-0.7em}
Overview of the problem
\begin{itemize}
\item Vessel stabilizer
\item {\color{blue}Why is a vessel stabilizer needed?}
\item What is Dacoma's current solution?
\item The objective
\item The approach
\end{itemize}
\end{frame}

\begin{frame}{Introductory}{Current solution\vphantom{(y}}
\vspace{-0.7em}
Overview of the problem
\begin{itemize}
\item Vessel stabilizer
\item Why is a vessel stabilizer needed?
\item {\color{blue}What is Dacoma's current solution?}
\item The objective
\item The approach
\end{itemize}
\end{frame}

\begin{frame}{Introductory}{Objective\vphantom{(y}}
\vspace{-0.7em}
Overview of the problem
\begin{itemize}
\item Vessel stabilizer
\item Why is a vessel stabilizer needed?
\item What is Dacoma's current solution?
\item {\color{blue}The objective}
\item The approach
\end{itemize}
\end{frame}

\begin{frame}{Introductory}{Approach\vphantom{(y}}
\vspace{-0.7em}
Overview of the problem
\begin{itemize}
\item Vessel stabilizer
\item Why is a vessel stabilizer needed?
\item What is Dacoma's current solution?
\item The objective
\item {\color{blue}The approach}
\end{itemize}
\end{frame}


\begin{frame}{Reinforcement Learning}{Overview\vphantom{(y}}
\vspace{-0.7em}
Overview:
\begin{itemize}
\item Classic Reinforcement Learning \& Terminology
\item The Approach
\item Issues and Shortcomings
\end{itemize}
\end{frame}

\begin{frame}{Reinforcement Learning}{Classic Reinforcement Learning \& Terminology\vphantom{(y}}
\vspace{-0.7em}
Terminology
\begin{itemize}
\item Agent
\item Environment
\item States, Statespace, Actions \& Actionspace
\item Reward and Reward function
\item Policy
\end{itemize}

Classic Reinforcement Learning (CRL)
\begin{itemize}
  \item Markov Decision Process (MDP)
  \item Problem Review
  \item Why CCRL does not fit on this problem \todo{Maybe colour with blue as well, maybe not}
\end{itemize}
\end{frame}

\begin{frame}{Reinforcement Learning}{The approach\vphantom{(y}}
\vspace{-0.7em}
Concepts:
\begin{itemize}
\item Policy Gradient Methods
\item Neural Networks
\item Optimizer
\end{itemize}
Implementation specifics:
\begin{itemize}
  \item Finite differences
  \item Reward \& Reward Function
\end{itemize}
\end{frame}

\begin{frame}{Reinforcement Learning}{The approach - Implementation specifics\vphantom{(y}}
\vspace{-0.7em}
Implementation specifics:
\begin{itemize}
  \item {\color{blue}Finite differences}
  \item Reward \& Reward Function
\end{itemize}

% \begin{algorithm}[H]
% \SetAlgoLined
% \KwResult{returns the reward and updates the gradient vector}
%  input: policy weights $\theta_h$ \& gradient vector $g_y$\;
%  \for{for n=0 to 500 do}{\;
%   generate policy variation $\epsilon \theta_n$\;
%   estimate $\dot{J_{+\epsilon}}_n \approx J(\theta_h + \frac{1}{2}\epsilon \theta_n)$ from roll-out \;
%   estimate  $\dot{J_{-\epsilon}}_n \approx J(\theta_h - \frac{1}{2}\epsilon \theta_n)$ from roll-out\;
%   compute $\Delta \dot{J_i} \approx J(\theta_h + \frac{1}{2}\epsilon  \theta_n)  - J(\theta_h - \frac{1}{2}\epsilon  \theta_n)$\;
%   compute $r += \frac{1}{2} (\cdot J_{+\epsilon}}_n + J_{-\epsilon}}_n) $\;
%  }
%  compute gradient estimate vector $g_y = (\epsilon \Theta ^T\epsilon \Theta)^{-1}\epsilon \Theta ^T \Delta \dot{J}$\;
%  return reward $R_{PD} = \frac{r}{500}$\;
%  \caption{Policy Gradient Estimation using finite differences altered to fit the actual implementation}
%  \label{al::alg2}
% \end{algorithm}
\end{frame}

\begin{frame}{Reinforcement Learning}{The approach - Implementation specifics\vphantom{(y}}
\vspace{-0.7em}
Implementation specifics:
\begin{itemize}
  \item Finite differences
  \item {\color{blue}Reward \& Reward Function}
\end{itemize}
\end{frame}

\begin{frame}{Introduction}{Curriculum for Reguleringsteknik (REG)\vphantom{(y}}
\vspace{-0.7em}
{\footnotesize Matematiske og grafiske metoder til syntese af {\color{red}lineære tidsinvariante systemer}:\footnote{{\tiny Based on https://fagbesk.sam.sdu.dk/?fag\_id=39673}}
\begin{itemize}
\item {\color{red}diskret og kontinuert tilstandsbeskrivelse}
\item analyse i tid og frekvens
\item stabilitet, reguleringshastighed, følsomhed og fejl
\item digitale PI, PID, LEAD og LAG regulatorer (serieregulatorer)
\item tilstandsregulering, pole-placement og tilstands-estimering (observer)
\item optimal regulering (least squares) og optimal tilstands-estimation (Kalman-filter)
\end{itemize}
\vspace{0.5em}
\textbf{Færdigheder}:\\
Efter gennemførelse af kurset kan den succesfulde studerende:
\begin{itemize}
\item kunne analysere, dimensionere og implementere såvel kontinuert som tidsdiskret regulering af lineære tidsinvariante og stokastiske systemer
\end{itemize}
\vspace{0.5em}
\textbf{Kompetencer}:\\
Efter gennemførelse af kurset kan den succesfulde studerende:
\begin{itemize}
\item anvende og implementere klassiske og moderne reguleringsteknikker for at kunne styre og regulere en robot hurtig og præcist
\end{itemize}}
\end{frame}
\end{document}
